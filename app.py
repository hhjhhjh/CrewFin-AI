"""
crew_senti_report_backend_v3.py

PRD: ë‹¤êµ­ì–´ ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ ì£¼ì‹ ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ v3.0 â€” Agent ë°±ì—”ë“œ (Telegram ì—°ë™ ì œì™¸)
ì‘ì„±: 2025-01-20 (v3.0)

ì´ íŒŒì¼ í•˜ë‚˜ë¡œ MVP íŒŒì´í”„ë¼ì¸(í‹°ì»¤ ë³€í™˜ âœ ë°ì´í„° ìˆ˜ì§‘ âœ ê°ì„± ë¶„ì„ âœ ì ìˆ˜/ì˜¨ë„ ì‚°ì¶œ âœ ë¦¬í¬íŠ¸ JSON/Markdown ìƒì„± âœ ì½˜ì†” ì¶œë ¥)ê¹Œì§€ ë™ì‘í•˜ë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

í•„ìš” í™˜ê²½ ë³€ìˆ˜(.env ê¶Œì¥):
  OPENAI_API_KEY=...
  TAVILY_API_KEY=...          # ì„ íƒ(ìˆìœ¼ë©´ Tavily MCP ì‚¬ìš©)
  SERPER_API_KEY=...          # ì„ íƒ(Fallback)
  NAVER_CLIENT_ID=...         # ì„ íƒ(Naver Search MCP)
  NAVER_CLIENT_SECRET=...     # ì„ íƒ(Naver Search MCP)

í•„ìš” íŒ¨í‚¤ì§€(ê¶Œì¥):
  pip install crewai crewai-tools openai python-dotenv cachetools pydantic python-dateutil rapidfuzz
  # MCP ì›ê²© ì—°ê²°ì€ ë³„ë„ ì„¤ì • í•„ìš”(ì‹¤ ì„œë¹„ìŠ¤ ì‹œ ë°˜ì˜). ì´ MVPëŠ” ì•ˆì „í•œ Fallback(ë”ë¯¸ ìˆ˜ì§‘ê¸°) í¬í•¨.

ì‹¤í–‰ ì˜ˆì‹œ:
  python crew_senti_report_backend_v3.py --company "ì—”ë¹„ë””ì•„" --lang ko
  python crew_senti_report_backend_v3.py --company "Samsung Electronics" --lang en

ì¶œë ¥:
  - í‘œì¤€ ì¶œë ¥ì— JSON ìš”ì•½ + Markdown ë¦¬í¬íŠ¸
  - í•„ìš” ì‹œ í•¨ìˆ˜ generate_report(company_input, language)ë¡œ ëª¨ë“ˆ ì„í¬íŠ¸ ì‚¬ìš© ê°€ëŠ¥

ì£¼ì˜:
  - ë³¸ íŒŒì¼ì€ Telegram/Webhook ì—†ì´ Agent ë°±ì—”ë“œë§Œ í¬í•¨í•©ë‹ˆë‹¤(ìš”ì²­í•˜ì‹  ë²”ìœ„).
  - Tavily MCP / Naver MCPëŠ” ì‹¤ì œ í‚¤Â·ì—°ê²° ì¡´ì¬ ì‹œì—ë§Œ í˜¸ì¶œí•˜ê³ , ì—†ìœ¼ë©´ Serper/ë”ë¯¸ë¡œ í´ë°±.
  - Reddit APIëŠ” ë¯¸í¬í•¨. í•„ìš” ì‹œ TODO ì£¼ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ ì—°ê²°.
"""
from __future__ import annotations

import os
import json
import time
import argparse
import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from datetime import datetime, timezone, timedelta
from dateutil import tz

from dotenv import load_dotenv
from cachetools import TTLCache
from pydantic import BaseModel, Field
from rapidfuzz import fuzz

# Added for Phase 2 real-data connectors
import requests
from urllib.parse import quote_plus
import random

# ============= ë¡œê¹… =============
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
log = logging.getLogger("crew-senti-backend")

# ============= í™˜ê²½ ë¡œë”© =============
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "").strip()
SERPER_API_KEY = os.getenv("SERPER_API_KEY", "").strip()
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "").strip()
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "").strip()

# ============= OpenAI SDK =============
try:
    from openai import OpenAI
    _openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
except Exception as e:
    _openai_client = None
    log.warning("OpenAI SDK ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨(ì‹¤í–‰ì€ ê°€ëŠ¥í•˜ë‚˜ LLM ê¸°ëŠ¥ ì œí•œ): %s", e)

MODEL_TICKER = "gpt-4o-mini"
MODEL_NLP    = "gpt-4o-mini"

# HTTP defaults
HTTP_TIMEOUT = 15
UA = "CrewFin-AI/1.0 (+https://example.local)"

# ============= CrewAI (ì—ì´ì „íŠ¸/íƒœìŠ¤í¬) =============
try:
    from crewai import Agent, Task, Crew, Process
except Exception as e:
    Agent = Task = Crew = Process = None
    log.warning("CrewAI ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨. fallback íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤: %s", e)

# ============= ê°„ë‹¨ ìºì‹œ =============
# íšŒì‚¬ëª… âœ í‹°ì»¤ ë³€í™˜ 1ì‹œê°„ TTL, ìˆ˜ì§‘ ê²°ê³¼ 10ë¶„ TTL
cache_ticker = TTLCache(maxsize=512, ttl=3600)
cache_collect = TTLCache(maxsize=512, ttl=600)

# ============= ìœ í‹¸ =============
KST = tz.gettz("Asia/Seoul")

def now_kst_iso() -> str:
    return datetime.now(KST).isoformat()

# ---------------- ì‹œì¥ ì˜¨ë„ ìŠ¤ì¼€ì¼ ----------------
TEMP_BUCKETS: List[Tuple[str, Tuple[int,int]]] = [
    ("Ice Cold", (0, 20)),
    ("Cool",     (21, 40)),
    ("Warm",     (41, 60)),
    ("Hot",      (61, 80)),
    ("Red Hot",  (81, 100)),
]

KO_TEMP_LABELS = {
    "Ice Cold": "ë§¤ìš° ì°¨ê°€ì›€",
    "Cool": "ì°¨ê°€ì›€",
    "Warm": "ë”°ëœ»í•¨",
    "Hot": "ëœ¨ê±°ì›€",
    "Red Hot": "ê³¼ì—´",
}

def score_to_temp_label(score: int) -> str:
    for label, (lo, hi) in TEMP_BUCKETS:
        if lo <= score <= hi:
            return label
    return "Warm"


# ---------------- ì–¸ì–´ ê°ì§€(ê°„ë‹¨) ----------------
# ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„  fastText/langdetect/LLM ë“±ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥
def simple_lang_detect(text: str) -> str:
    # í•œê¸€ ì¡´ì¬ ë¹„ìœ¨ ê¸°ë°˜
    korean_ratio = sum(0xAC00 <= ord(ch) <= 0xD7A3 for ch in text) / max(len(text),1)
    return "ko" if korean_ratio > 0.1 else "en"

# ============= í‹°ì»¤ ë³€í™˜ =============
class TickerResult(BaseModel):
    success: bool = True
    primary_ticker: Optional[str] = None
    market: Optional[str] = None  # "US" | "KR"
    company_name: Optional[str] = None
    alternatives: List[Dict[str, Any]] = Field(default_factory=list)
    error_message: Optional[str] = None

SYSTEM_TICKER_PROMPT = (
    "ë‹¹ì‹ ì€ ì£¼ì‹ í‹°ì»¤ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ íšŒì‚¬ëª…ì„ ì •í™•í•œ ì£¼ì‹ í‹°ì»¤ë¡œ ë³€í™˜í•˜ê³  ì‹œì¥ì„ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤.\n"
    "ê·œì¹™:\n"
    "1) ë¯¸êµ­ ì‹œì¥: í‹°ì»¤ë§Œ ë°˜í™˜(ì˜ˆ: NVDA, AAPL)\n"
    "2) í•œêµ­ ì‹œì¥: .KS ë˜ëŠ” .KQ ì ‘ë¯¸ì‚¬(ì˜ˆ: 005930.KS, 035420.KQ)\n"
    "3) ë¶ˆí™•ì‹¤ ì‹œ ìƒìœ„ 3ê°œ í›„ë³´ì™€ confidence í¬í•¨\n"
    "4) í•œêµ­ì–´ ì…ë ¥ ì‹œ í•œêµ­ ì‹œì¥ ìš°ì„ , ì˜ì–´ ì…ë ¥ ì‹œ ë¯¸êµ­ ì‹œì¥ ìš°ì„ \n"
    "5) ë°˜ë“œì‹œ JSONìœ¼ë¡œë§Œ ì‘ë‹µ"
)

def convert_company_name_to_ticker(company_name: str, language: str) -> TickerResult:
    key = f"{language}:{company_name.strip().lower()}"
    if key in cache_ticker:
        return cache_ticker[key]

    # LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œ í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ
    if _openai_client:
        user_prompt = json.dumps({
            "company_name": company_name,
            "language": language,
            "priority": "KR>US" if language == "ko" else "US>KR"
        }, ensure_ascii=False)
        try:
            resp = _openai_client.chat.completions.create(
                model=MODEL_TICKER,
                messages=[
                    {"role": "system", "content": SYSTEM_TICKER_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.0,
            )
            content = resp.choices[0].message.content.strip()
            data = json.loads(content)
            # í‘œì¤€í™”
            out = TickerResult(
                success=bool(data.get("success", True)),
                primary_ticker=data.get("primary_ticker"),
                market=data.get("market"),
                company_name=data.get("company_name"),
                alternatives=data.get("alternatives", []),
                error_message=data.get("error_message"),
            )
            cache_ticker[key] = out
            return out
        except Exception as e:
            log.warning("LLM í‹°ì»¤ ë³€í™˜ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± í´ë°±: %s", e)

    # í´ë°±(ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±): ëŒ€í‘œ ì¼€ì´ìŠ¤ ë§¤í•‘
    canonical = {
        "ì—”ë¹„ë””ì•„": ("NVDA", "US", "NVIDIA Corporation"),
        "nvidia": ("NVDA", "US", "NVIDIA Corporation"),
        "ì‚¼ì„±ì „ì": ("005930.KS", "KR", "Samsung Electronics Co., Ltd."),
        "samsung electronics": ("005930.KS", "KR", "Samsung Electronics Co., Ltd."),
        "apple": ("AAPL", "US", "Apple Inc."),
        "ì• í”Œ": ("AAPL", "US", "Apple Inc."),
    }
    key2 = company_name.strip().lower()
    if key2 in canonical:
        t, m, n = canonical[key2]
        out = TickerResult(success=True, primary_ticker=t, market=m, company_name=n,
                           alternatives=[{"ticker": t, "market": m, "confidence": 0.8}])
    else:
        # fuzzyë¡œ ì‚¼ì„±ì „ì/ì—”ë¹„ë””ì•„/ì• í”Œ ê·¼ì ‘ ì²˜ë¦¬
        best = max(canonical.keys(), key=lambda k: fuzz.token_set_ratio(k, key2))
        score = fuzz.token_set_ratio(best, key2)
        if score >= 70:
            t, m, n = canonical[best]
            out = TickerResult(success=True, primary_ticker=t, market=m, company_name=n,
                               alternatives=[{"ticker": t, "market": m, "confidence": score/100}])
        else:
            out = TickerResult(success=False, error_message="íšŒì‚¬ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    cache_ticker[key] = out
    return out

# ============= ë°ì´í„° ìˆ˜ì§‘ê¸°(ë‰´ìŠ¤/ì»¤ë®¤ë‹ˆí‹°) =============
class CollectedItem(BaseModel):
    source: str                 # e.g., "news", "reddit", "naver_cafe"
    title: str
    url: Optional[str] = None
    published_at: Optional[str] = None  # ISO8601
    content: Optional[str] = None       # ê¸°ì‚¬ ë³¸ë¬¸ ë˜ëŠ” ìš”ì•½

class CollectorOutput(BaseModel):
    news: List[CollectedItem] = Field(default_factory=list)
    community: List[CollectedItem] = Field(default_factory=list)


def _dummy_news(company: str) -> List[CollectedItem]:
    now = now_kst_iso()
    return [
        CollectedItem(source="news", title=f"{company} ì‹ ì œí’ˆ ì¶œì‹œ ê¸°ëŒ€", url=None, published_at=now,
                      content=f"{company} ê´€ë ¨ ì‹ ì œí’ˆ ë£¨ë¨¸ì™€ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€ê°ì´ ì»¤ë®¤ë‹ˆí‹°ì™€ ë‰´ìŠ¤ì—ì„œ í™•ì‚°."),
        CollectedItem(source="news", title=f"{company} ê²½ìŸ ì‹¬í™”", url=None, published_at=now,
                      content="ê²½ìŸì‚¬ ê°€ê²© ì¸í•˜ ë° ì‹œì¥ ì§„ì…ìœ¼ë¡œ ì ìœ ìœ¨ ê²½ìŸì´ ì‹¬í™”."),
    ]


def _dummy_community(company: str) -> List[CollectedItem]:
    now = now_kst_iso()
    return [
        CollectedItem(source="reddit", title=f"Long {company}?", published_at=now,
                      content="ì‹ ì œí’ˆ ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ë£¨ë¨¸, ë‹¨ê¸° ëª¨ë©˜í…€ ê¸°ëŒ€."),
        CollectedItem(source="naver_cafe", title=f"{company} ê·œì œ ìš°ë ¤", published_at=now,
                      content="ë°˜ë…ì  ì´ìŠˆ ê°€ëŠ¥ì„± ì–¸ê¸‰, ë³€ë™ì„± ì£¼ì˜ ì˜ê²¬."),
    ]


def _serper_search_news(company_name: str, days: int, max_results: int) -> List[CollectedItem]:
    items: List[CollectedItem] = []
    try:
        from crewai_tools import SerperDevTool
        tool = SerperDevTool()  # SERPER_API_KEYëŠ” envì—ì„œ ìë™ ì¸ì‹
        q = f"{company_name} stock news last {days} days"
        res = tool.run(q)
        if isinstance(res, dict):
            for i in res.get("news", [])[:max_results]:
                items.append(CollectedItem(
                    source="news",
                    title=i.get("title", ""),
                    url=i.get("link"),
                    published_at=i.get("date"),
                    content=i.get("snippet")
                ))
    except Exception as e:
        log.warning("Serper ê²€ìƒ‰ ì‹¤íŒ¨: %s", e)
    return items


def _tavily_search_news(company_name: str, days: int, max_results: int) -> List[CollectedItem]:
    """Tavily REST (Bearer auth). Docs:
    https://docs.tavily.com/documentation/api-reference/endpoint/search
    """
    items: List[CollectedItem] = []
    api_key = TAVILY_API_KEY
    if not api_key:
        return items

    url = "https://api.tavily.com/search"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",   # âœ… í•µì‹¬: Bearer í—¤ë”
        "User-Agent": UA,
    }
    payload = {
        "query": f"{company_name} stock news",
        "topic": "finance",
        "days": days,
        "max_results": max_results,
        "include_raw_content": True,
        "search_depth": "basic",
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=HTTP_TIMEOUT)
        if r.status_code != 200:
            log.warning("Tavily HTTP %s: %s", r.status_code, r.text[:200])
            return items
        data = r.json()
        results = data.get("results", [])
        for res in results[:max_results]:
            items.append(CollectedItem(
                source="news",
                title=res.get("title") or res.get("name") or "",
                url=res.get("url"),
                published_at=res.get("published_date") or res.get("date"),
                content=(res.get("raw_content") or res.get("content") or res.get("snippet"))
            ))
    except Exception as e:
        log.warning("Tavily ì˜ˆì™¸: %s", e)
    return items



def _naver_search(endpoint: str, query: str, display: int = 7, sort: str = "date") -> List[Dict[str, Any]]:
    url = f"https://openapi.naver.com/v1/search/{endpoint}.json?query={quote_plus(query)}&display={display}&sort={sort}"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": UA,
    }
    try:
        r = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT)
        if r.status_code == 200:
            return r.json().get("items", [])
        log.warning("Naver %s ì‹¤íŒ¨(%s): %s", endpoint, r.status_code, r.text[:200])
    except Exception as e:
        log.warning("Naver %s ì˜ˆì™¸: %s", endpoint, e)
    return []


def _naver_collect(company_name: str, max_news: int = 7, cafe_n: int = 10, blog_n: int = 5) -> Tuple[List[CollectedItem], List[CollectedItem]]:
    news_items: List[CollectedItem] = []
    comm_items: List[CollectedItem] = []
    for it in _naver_search("news", company_name, display=max_news, sort="date")[:max_news]:
        news_items.append(CollectedItem(
            source="news",
            title=it.get("title",""),
            url=it.get("link"),
            published_at=None,
            content=it.get("description")
        ))
    for it in _naver_search("cafearticle", company_name, display=cafe_n, sort="date")[:cafe_n]:
        comm_items.append(CollectedItem(
            source="naver_cafe",
            title=it.get("title",""),
            url=it.get("link"),
            published_at=None,
            content=it.get("description")
        ))
    for it in _naver_search("blog", f"{company_name} ì£¼ì‹", display=blog_n, sort="date")[:blog_n]:
        comm_items.append(CollectedItem(
            source="naver_blog",
            title=it.get("title",""),
            url=it.get("link"),
            published_at=None,
            content=it.get("description")
        ))
    return news_items, comm_items


def _reddit_recent_posts(sub: str, query: str, limit: int = 5) -> List[CollectedItem]:
    items: List[CollectedItem] = []
    url = f"https://www.reddit.com/r/{sub}/search.json?q={quote_plus(query)}&restrict_sr=1&sort=new&t=week&limit={limit}"
    try:
        r = requests.get(url, headers={"User-Agent": UA}, timeout=HTTP_TIMEOUT)
        if r.status_code == 200:
            data = r.json().get("data", {}).get("children", [])
            for ch in data[:limit]:
                d = ch.get("data", {})
                items.append(CollectedItem(
                    source="reddit",
                    title=d.get("title",""),
                    url=f"https://www.reddit.com{d.get('permalink','')}",
                    published_at=datetime.fromtimestamp(d.get("created_utc", 0), tz=KST).isoformat() if d.get("created_utc") else None,
                    content=d.get("selftext") or d.get("title")
                ))
        else:
            log.warning("Reddit %s ì‹¤íŒ¨(%s)", sub, r.status_code)
    except Exception as e:
        log.warning("Reddit ì˜ˆì™¸(%s): %s", sub, e)
    return items


def collect_data(company_name: str, market: str, days: int = 7, max_results_news: int = 7) -> CollectorOutput:
    key = f"collect:{company_name}:{market}:{days}:{max_results_news}"
    if key in cache_collect:
        return cache_collect[key]

    items_news: List[CollectedItem] = []
    items_comm: List[CollectedItem] = []
    used_any = False

    # 1) Tavily ìš°ì„ 
    if TAVILY_API_KEY:
        tv_news = _tavily_search_news(company_name, days=days, max_results=max_results_news)
        if tv_news:
            items_news.extend(tv_news)
            used_any = True

    # 2) Serper Fallback
    if not items_news and SERPER_API_KEY:
        sp_news = _serper_search_news(company_name, days=days, max_results=max_results_news)
        if sp_news:
            items_news.extend(sp_news)
            used_any = True

    # 3) Naver (KR ë‰´ìŠ¤/ì¹´í˜/ë¸”ë¡œê·¸)
    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        nv_news, nv_comm = _naver_collect(company_name, max_news=max_results_news, cafe_n=10, blog_n=5)
        if nv_news:
            # ê°„ë‹¨ ì¤‘ë³µ ì œê±°
            seen = set()
            for it in nv_news:
                k = (it.title, it.url)
                if k not in seen:
                    items_news.append(it)
                    seen.add(k)
            used_any = True or used_any
        if nv_comm:
            items_comm.extend(nv_comm)
            used_any = True or used_any

    # 4) Reddit (ë¯¸êµ­ ì»¤ë®¤ë‹ˆí‹°)
    for s in ["wallstreetbets", "stocks", "investing"]:
        items_comm.extend(_reddit_recent_posts(s, company_name, limit=5))
    if items_comm:
        used_any = True

    # 5) ë”ë¯¸ í´ë°±
    if not used_any:
        items_news = _dummy_news(company_name)
        items_comm = _dummy_community(company_name)

    out = CollectorOutput(news=items_news[:max_results_news], community=items_comm)
    cache_collect[key] = out
    return out

# ============= ê°ì„± ë¶„ì„ =============
class SentimentResult(BaseModel):
    label: str   # "positive"|"negative"|"neutral"
    confidence: float


SENTIMENT_SYSTEM = (
    "ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤/ì»¤ë®¤ë‹ˆí‹° í¬ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ë¶„ë¥˜í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê²°ê³¼ëŠ” JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n"
    "ë¼ë²¨ì€ positive/negative/neutral ì¤‘ í•˜ë‚˜. í™•ë¥ (confidence)ì€ 0~1.")


def analyze_sentiment_batch(texts: List[str]) -> List[SentimentResult]:
    results: List[SentimentResult] = []
    if _openai_client:
        try:
            # ê°„ë‹¨í•œ batched í˜¸ì¶œ: í”„ë¡¬í”„íŠ¸ ë‚´ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
            user_payload = {"items": texts}
            resp = _openai_client.chat.completions.create(
                model=MODEL_NLP,
                messages=[
                    {"role": "system", "content": SENTIMENT_SYSTEM},
                    {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
                ],
                temperature=0.0,
            )
            content = resp.choices[0].message.content.strip()
            data = json.loads(content)
            # ê¸°ëŒ€ í˜•ì‹: [{"label": "positive", "confidence": 0.92}, ...]
            for d in data:
                results.append(SentimentResult(label=d.get("label","neutral"), confidence=float(d.get("confidence",0.5))))
            return results
        except Exception as e:
            log.warning("LLM ê°ì„±ë¶„ì„ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ í´ë°±: %s", e)

    # í´ë°±: ê·¹ë‹¨ì  í‚¤ì›Œë“œ ê·œì¹™(ì•„ì£¼ ë‹¨ìˆœ)
    pos_kw = ["great","good","positive","ìƒìŠ¹","í˜¸ì¬","ê¸°ëŒ€","outperform","beat"]
    neg_kw = ["bad","negative","í•˜ë½","ì•…ì¬","ìš°ë ¤","ì†Œì†¡","antitrust","ban"]
    for t in texts:
        t_low = (t or "").lower()
        pos = any(k in t_low for k in pos_kw)
        neg = any(k in t_low for k in neg_kw)
        if pos and not neg:
            results.append(SentimentResult(label="positive", confidence=0.7))
        elif neg and not pos:
            results.append(SentimentResult(label="negative", confidence=0.7))
        elif pos and neg:
            results.append(SentimentResult(label="neutral", confidence=0.55))
        else:
            results.append(SentimentResult(label="neutral", confidence=0.5))
    return results

# ============= ì ìˆ˜/ì˜¨ë„ & í‚¤ í…Œë§ˆ =============
class MarketTemperature(BaseModel):
    label: str
    score: int
    scale: str = "0(Ice Cold) ~ 100(Red Hot)"

class SentimentSummary(BaseModel):
    positive: int
    negative: int
    neutral: int

class CommunitySentimentAnalysis(BaseModel):
    market_temperature: MarketTemperature
    sentiment_summary: SentimentSummary
    data_sources: Dict[str, int]
    key_themes: List[str]


def aggregate_sentiment(news: List[CollectedItem], comm: List[CollectedItem]) -> CommunitySentimentAnalysis:
    texts = [(n.content or n.title or "") for n in news] + [(c.content or c.title or "") for c in comm]
    sents = analyze_sentiment_batch(texts)

    pos = sum(1 for s in sents if s.label == "positive")
    neg = sum(1 for s in sents if s.label == "negative")
    neu = sum(1 for s in sents if s.label == "neutral")

    # ì ìˆ˜: (pos - neg) / total âœ [0..100]ë¡œ ë§¤í•‘ (ì¤‘ë¦½ 50)
    total = max(pos + neg + neu, 1)
    raw = (pos - neg) / total  # -1..+1
    score = int(round(50 + raw * 50))
    score = max(0, min(100, score))
    label = score_to_temp_label(score)

    # í‚¤ í…Œë§ˆ ì¶”ì¶œ(ì•„ì£¼ ë‹¨ìˆœ ìš”ì•½)
    key_themes: List[str] = []
    if pos:
        key_themes.append("ì‹ ì œí’ˆ/ê¸°ìˆ  ëª¨ë©˜í…€ì— ëŒ€í•œ ê¸°ëŒ€ê° (ê¸ì •)")
    if neg:
        key_themes.append("ê²½ìŸ ì‹¬í™”/ê·œì œ ì´ìŠˆì— ëŒ€í•œ ìš°ë ¤ (ë¶€ì •)")
    if neu:
        key_themes.append("ì¤‘ë¦½ì  ê´€ë§ì„¸ ë° ì‹¤ì  í™•ì¸ ëŒ€ê¸° (ì¤‘ë¦½)")
    if not key_themes:
        key_themes = ["ë°ì´í„° ì œí•œìœ¼ë¡œ ìœ ì˜ë¯¸í•œ í…Œë§ˆë¥¼ ë„ì¶œí•˜ê¸° ì–´ë ¤ì›€ (ì¤‘ë¦½)"]

    label_en = label
    label_ko = KO_TEMP_LABELS.get(label_en, label_en)
    return CommunitySentimentAnalysis(
        market_temperature=MarketTemperature(label=f"{label_en} / {label_ko}", score=score),
        sentiment_summary=SentimentSummary(positive=pos, negative=neg, neutral=neu),
        data_sources={"news_count": len(news), "community_posts_count": len(comm)},
        key_themes=key_themes[:3],
    )

# ============= ë¦¬í¬íŠ¸ ìŠ¤í‚¤ë§ˆ & í¬ë§·í„° =============
class AnalystCommentary(BaseModel):
    market_reaction_interpretation: str
    investment_perspective: str
    risk_factors: List[str]
    opportunity_factors: List[str]

class ReportModel(BaseModel):
    report_generated_at: str
    language: str
    company_info: Dict[str, Any]
    community_sentiment_analysis: CommunitySentimentAnalysis
    analyst_commentary: AnalystCommentary
    overall_summary: str
    disclaimer: str


def draft_analyst_commentary(lang: str, csa: CommunitySentimentAnalysis, company_name: str) -> AnalystCommentary:
    # LLM ê°€ëŠ¥ ì‹œ ë” ê³ ê¸‰ ì„œìˆ . ì—¬ê¸°ì„  ê·œì¹™ ê¸°ë°˜ í…œí”Œë¦¿.
    warmline = {
        "ko": f"ì»¤ë®¤ë‹ˆí‹°ì™€ ë‰´ìŠ¤ì—ì„œ {company_name}ì— ëŒ€í•œ ê¸°ëŒ€ê°ê³¼ ìš°ë ¤ê°€ í˜¼ì¬í•©ë‹ˆë‹¤.",
        "en": f"Across news and communities, sentiment on {company_name} mixes optimism with caution.",
    }
    investline = {
        "ko": "ë‹¨ê¸° ëª¨ë©˜í…€ì€ ê¸ì •ì ì¼ ìˆ˜ ìˆìœ¼ë‚˜, ì¤‘ì¥ê¸° ë¦¬ìŠ¤í¬ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "en": "Shortâ€‘term momentum may be constructive, while mediumâ€‘term risks warrant monitoring.",
    }
    risks = ["ê²½ìŸ ì‹¬í™”", "ê·œì œ/ë°˜ë…ì  ì´ìŠˆ", "ë§¤í¬ë¡œ ë³€ë™ì„± í™•ëŒ€"]
    opps  = ["ì‹ ì œí’ˆ/ê¸°ìˆ  ì£¼ë„ ëª¨ë©˜í…€", "AI ìˆ˜ìš” í™•ì¥", "ìƒíƒœê³„ ê°•í™”"]
    return AnalystCommentary(
        market_reaction_interpretation=warmline.get(lang, warmline["en"]),
        investment_perspective=investline.get(lang, investline["en"]),
        risk_factors=risks,
        opportunity_factors=opps,
    )


def report_to_markdown(r: ReportModel) -> str:
    lang = r.language
    ci = r.company_info
    csa = r.community_sentiment_analysis
    ac = r.analyst_commentary

    if lang == "ko":
        md = []
        md.append(f"### ğŸ“Š ì»¤ë®¤ë‹ˆí‹° ê°ì„± ì¢…í•© ë¦¬í¬íŠ¸ â€” {ci.get('name')} ({ci.get('ticker')})")
        md.append(f"- ìƒì„±ì‹œê°(KST): {r.report_generated_at}")
        md.append("")
        md.append("**ì‹œì¥ ì˜¨ë„(Temperature)**")
        md.append(f"- ë ˆì´ë¸”: {csa.market_temperature.label}")
        md.append(f"- ì ìˆ˜: {csa.market_temperature.score} / 100")
        md.append("")
        md.append("**ê°ì„± ìš”ì•½**")
        ss = csa.sentiment_summary
        md.append(f"- ê¸ì • {ss.positive} / ë¶€ì • {ss.negative} / ì¤‘ë¦½ {ss.neutral}")
        md.append(f"- ë°ì´í„° ì¶œì²˜: ë‰´ìŠ¤ {csa.data_sources['news_count']}ê°œ, ì»¤ë®¤ë‹ˆí‹° {csa.data_sources['community_posts_count']}ê°œ")
        md.append("")
        md.append("**í•µì‹¬ í…Œë§ˆ(Top 2-3)**")
        for t in csa.key_themes:
            md.append(f"- {t}")
        md.append("")
        md.append("**ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸**")
        md.append(f"- ì‹œì¥ ë°˜ì‘ í•´ì„: {ac.market_reaction_interpretation}")
        md.append(f"- íˆ¬ì ê´€ì : {ac.investment_perspective}")
        md.append("- ë¦¬ìŠ¤í¬ ìš”ì¸:")
        for x in ac.risk_factors:
            md.append(f"  - {x}")
        md.append("- ê¸°íšŒ ìš”ì¸:")
        for x in ac.opportunity_factors:
            md.append(f"  - {x}")
        md.append("")
        md.append(f"**í•œì¤„ ìš”ì•½**  ")
        md.append(f"{r.overall_summary}")
        md.append("")
        md.append(
            "> ë³¸ ë¦¬í¬íŠ¸ëŠ” AIê°€ ìƒì„±í•œ ì •ë³´ë¡œì„œ ì¼ë°˜ì  ì°¸ê³ ìš©ì´ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤. íˆ¬ì ê²°ì •ì€ ì‚¬ìš©ì ì±…ì„ì…ë‹ˆë‹¤."
        )
        return "\n".join(md)
    else:
        md = []
        md.append(f"### ğŸ“Š Community Sentiment Report â€” {ci.get('name')} ({ci.get('ticker')})")
        md.append(f"- Generated (KST): {r.report_generated_at}")
        md.append("")
        md.append("**Market Temperature**")
        md.append(f"- Label: {csa.market_temperature.label}")
        md.append(f"- Score: {csa.market_temperature.score} / 100")
        md.append("")
        md.append("**Sentiment Summary**")
        ss = csa.sentiment_summary
        md.append(f"- Positive {ss.positive} / Negative {ss.negative} / Neutral {ss.neutral}")
        md.append(f"- Data sources: News {csa.data_sources['news_count']}, Community {csa.data_sources['community_posts_count']}")
        md.append("")
        md.append("**Key Themes (Top 2-3)**")
        for t in csa.key_themes:
            md.append(f"- {t}")
        md.append("")
        md.append("**Analyst Commentary**")
        md.append(f"- Market Interpretation: {ac.market_reaction_interpretation}")
        md.append(f"- Investment Perspective: {ac.investment_perspective}")
        md.append("- Risk Factors:")
        for x in ac.risk_factors:
            md.append(f"  - {x}")
        md.append("- Opportunity Factors:")
        for x in ac.opportunity_factors:
            md.append(f"  - {x}")
        md.append("")
        md.append("**TL;DR**  ")
        md.append(r.overall_summary)
        md.append("")
        md.append(
            "> This report is AIâ€‘generated and for informational purposes only. Not investment advice."
        )
        return "\n".join(md)

# ============= ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ =============

def generate_report(company_input: str, language: Optional[str] = None) -> Tuple[ReportModel, str]:
    lang = language or simple_lang_detect(company_input)

    # 1) í‹°ì»¤ ë³€í™˜
    ticker_info = convert_company_name_to_ticker(company_input, lang)
    if not ticker_info.success or not ticker_info.primary_ticker:
        # ì‹¤íŒ¨ ë¦¬í¬íŠ¸
        ci = {
            "name": company_input,
            "ticker": None,
            "market": None,
            "summary": None,
        }
        csa = CommunitySentimentAnalysis(
            market_temperature=MarketTemperature(label="Warm / ë”°ëœ»í•¨", score=50),
            sentiment_summary=SentimentSummary(positive=0, negative=0, neutral=0),
            data_sources={"news_count": 0, "community_posts_count": 0},
            key_themes=["í‹°ì»¤ ì‹ë³„ ì‹¤íŒ¨ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€"],
        )
        ac = draft_analyst_commentary(lang, csa, company_input)
        report = ReportModel(
            report_generated_at=now_kst_iso(),
            language=lang,
            company_info=ci,
            community_sentiment_analysis=csa,
            analyst_commentary=ac,
            overall_summary=("ì…ë ¥í•˜ì‹  íšŒì‚¬ëª…ì„ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”." if lang=="ko"
                             else "Could not resolve the company. Please provide a more specific name."),
            disclaimer=("ë³¸ ë¦¬í¬íŠ¸ëŠ” AIì— ì˜í•´ ìƒì„±ëœ ì •ë³´ë¡œì„œ ì¼ë°˜ì  ì°¸ê³ ìš©ì´ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤." if lang=="ko" else
                        "This report is AIâ€‘generated and for informational purposes only. Not investment advice."),
        )
        md = report_to_markdown(report)
        return report, md

    # 2) ë°ì´í„° ìˆ˜ì§‘
    coll = collect_data(ticker_info.company_name or company_input, ticker_info.market or "US")

    # 3) ê°ì„± ì§‘ê³„/ì˜¨ë„
    csa = aggregate_sentiment(coll.news, coll.community)

    # 4) ì• ë„ë¦¬ìŠ¤íŠ¸ ì½”ë©˜íŠ¸/ìš”ì•½
    ac = draft_analyst_commentary(lang, csa, ticker_info.company_name or company_input)

    overall = (
        f"ì‹ ì œí’ˆ ê¸°ëŒ€ì™€ ì—…í™© í™•ì¥ì— ëŒ€í•œ ê¸ì •ê³¼, ê²½ìŸ/ê·œì œ ìš°ë ¤ê°€ ê· í˜•ì„ ì´ë£¨ëŠ” {csa.market_temperature.label} êµ­ë©´ì…ë‹ˆë‹¤."
        if lang=="ko" else
        f"A {csa.market_temperature.label} backdrop where product momentum and industry expansion are tempered by competition/regulatory risks."
    )

    report = ReportModel(
        report_generated_at=now_kst_iso(),
        language=lang,
        company_info={
            "name": ticker_info.company_name or company_input,
            "ticker": ticker_info.primary_ticker,
            "market": ticker_info.market,
            "summary": None,
        },
        community_sentiment_analysis=csa,
        analyst_commentary=ac,
        overall_summary=overall,
        disclaimer=("ë³¸ ë¦¬í¬íŠ¸ëŠ” AIì— ì˜í•´ ìƒì„±ëœ ì •ë³´ë¡œì„œ ì¼ë°˜ì  ì°¸ê³ ìš©ì´ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤. íˆ¬ì ê²°ì •ì€ ì‚¬ìš©ì ì±…ì„ì…ë‹ˆë‹¤." if lang=="ko" else
                    "This report is AIâ€‘generated for general information only and is not investment advice."),
    )
    md = report_to_markdown(report)
    return report, md

# ============= CLI ì§„ì…ì  =============
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Crew Sentiment Report Backend (Agentâ€‘only)")
    parser.add_argument("--company", required=True, help="íšŒì‚¬ëª… ë˜ëŠ” ì¢…ëª©ëª… (ì˜ˆ: ì—”ë¹„ë””ì•„, ì‚¼ì„±ì „ì, Apple)")
    parser.add_argument("--lang", default=None, help="ì‘ë‹µ ì–¸ì–´(ko|en). ë¯¸ì§€ì • ì‹œ ìë™ ê°ì§€")
    parser.add_argument("--pretty", action="store_true", help="Pretty JSON output")
    args = parser.parse_args()

    rep, md = generate_report(args.company, args.lang)

    print("=== JSON ===")
    if args.pretty:
        print(json.dumps(rep.model_dump(), ensure_ascii=False, indent=2))
    else:
        print(json.dumps(rep.model_dump(), ensure_ascii=False))

    print("=== Markdown ===")
    print(md)
